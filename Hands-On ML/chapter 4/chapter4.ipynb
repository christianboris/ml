{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Training Models\n",
    "General overview of models and how to train them. Code based on the book, chapter IV \"Training Models\".\n",
    "\n",
    "Modified to experiment with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "y = 4 + 3 * X + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE](MSE.png \"MSE\")  \n",
    "MSE  \n",
    "  \n",
    "![Vectorized MSE](Vectorized.png \"Vectorized MSE\")  \n",
    "Vectorized MSE  \n",
    "  \n",
    "![Derivative of MSE](Derivative.png \"Derivative of MSE\")  \n",
    "Derivative  \n",
    "  \n",
    "![Find min](Minimize.png \"Minimize Derivative\")  \n",
    "Minimize derivative  \n",
    "  \n",
    "![Solving](Solving.png \"Solving derivative\")  \n",
    "Solving  \n",
    "  \n",
    "![Normal Equation](NormalEquation.png \"Result: Normal Equation\")  \n",
    "Solution/Normal Equation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "X_b = add_dummy_feature(X) # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21509616]), array([[2.77011339]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or more ML-like implementation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient Descent is more flexible. Normal equation is computationally complex (O(n<sup>2.4</sup>) - O(n<sup>3</sup>)) and might not exist for all problems (i.e. non-linear relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Find min](Minimize.png \"Minimize Derivative\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21509616],\n",
       "       [2.77011339]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch Gradient Descent\n",
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1) # randomly initialized model\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    gradients = 2/len(X_b) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.21076011],\n",
       "       [2.74856079]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "n_iterations = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "# Define a learning schedule to decrease the learning rate over time\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1) # randomly initialized model\n",
    "\n",
    "for epoch in range(n_iterations):\n",
    "    for i in range(len(X_b)):\n",
    "        random_index = np.random.randint(len(X_b))\n",
    "        xi = X_b[random_index:random_index+1] # pick a random instance\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * len(X_b) + i)\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.21278812]), array([2.77270267]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More ML-like implementation\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000,tol=1e-5, penalty=None, eta0=0.01,\n",
    "                       n_iter_no_change=100, random_state=42)\n",
    "# tol: tolerance for stopping criteria\n",
    "# penalty: regularization term (None for no regularization)\n",
    "# eta0: initial learning rate\n",
    "# n_iter_no_change: number of iterations with no improvement before stopping\n",
    "\n",
    "sgd_reg.fit(X, y.ravel()) # ravel() flattens y into a 1D array\n",
    "\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nonlinear data\n",
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3 # random values between -3 and 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) # quadratic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.78134581]), array([[0.93366893, 0.56456263]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Transform training data to a 2nd degree polynomial adding second-degree polynomial (but no bias term (x^0))\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression / L2 Regularization\n",
    "Addition of regularization term to MSE   \n",
    "![\"Regularization Term\"](Rigeterm.png)  \n",
    "  \n",
    "This equals the square of the L2 Norm  \n",
    "![\"L2 Norm\"](L2Norm.png)  \n",
    "L2 Norm  \n",
    "\n",
    "  \n",
    "This leads to full MSE as  \n",
    "![\"MSE with L2 Regularization\"](rigeregression.png)  \n",
    "  \n",
    "Or (easier/more famous) as:  \n",
    "![\"MSE with L2 Regularization\"](rige2.png)  \n",
    "  \n",
    "  \n",
    "Again, this could be solved in closed-form (via Cholesky matrix factorization) or Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.78211323]), array([0.93420757, 0.56518136]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(penalty='l2', alpha=0.1 / m, tol=None,\n",
    "                          max_iter=1000, eta0=0.01, random_state=42)\n",
    "# penalty: Regularization term (l2 = Ridge, l1 = Lasso)\n",
    "# Because l2 does not divide the regularization term by m, it has to be added\n",
    "\n",
    "sgd_reg.fit(X_poly, y.ravel()) # ravel() flattens y into a 1D array\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression / L1 Regularization\n",
    "\n",
    "Lasso Regression adds the L1 Norm to the MSE (but not squared like Ridge Regression)  \n",
    "![\"L1 Norm\"](L1Norm.png)  \n",
    "  \n",
    "This leads to  \n",
    "![\"Lasso MSE\"](Lasso.png)  \n",
    "  \n",
    "Note:\n",
    "* Regularization term is often multiplied by 2 (because of derivative of MSE that leads to leading 2)\n",
    "* Division by m/size of dataset often done.\n",
    "* Combination of both also possible.\n",
    "\n",
    "However, there is no fixed rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code just as above, only with penalty='l1' for Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "Combination of Ridge Regression and Lasso Regression  \n",
    "![Elastic Net Regression](ElasticNet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.55886814]), array([0.81485127]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "# l1_ratio: ratio of l1 and l2 regularization terms\n",
    "elastic_net.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Ridge Regression:\n",
    "* Good default\n",
    "* When expected that (almost) all features relevant or cross-correlation between features\n",
    "\n",
    "--> Stabilizes model by reducing weight of coefficients, but will not set them to zero  \n",
    "  \n",
    "Lasso Regression:\n",
    "* When expected that only few features are relevant\n",
    "\n",
    "--> Can set weights of coefficients to zero  \n",
    "  \n",
    "Elastic Net:\n",
    "* Especially useful when more features than observations or when features correlated\n",
    "* Preferred over Lasso because Lasso might be erratic in cases above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
